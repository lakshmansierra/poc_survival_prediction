{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "37puETfgRzzg"
      },
      "source": [
        "# lifelines - predictive maintenance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "N-qiINBQSK2g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pyspark_df = spark.table(\"sd_bdc_demo.sarima_time_series_forecasting.1_service_now_survival_data\")\n",
        "# pyspark_df.display()\n",
        "# df = pyspark_df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"2_service_now_lifelines_updated_data/2_service_now_lifelines_updated_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Asset_Number  start  stop  event  Category      usage  temperature  \\\n",
            "0        100080      0    14      0  Security  89.242856    21.072078   \n",
            "1        100080     14    17      1  Security  18.028311    52.449810   \n",
            "2        100080     17    19      1  Security  56.002279    75.497191   \n",
            "3        100080     19    21      0  Security  13.719146    12.130803   \n",
            "4        100080     21    25      0  Security  34.996827    10.222960   \n",
            "\n",
            "        load  health_bar  \n",
            "0  18.921408   75.926079  \n",
            "1  30.837617   46.889084  \n",
            "2  14.388191   46.312371  \n",
            "3  54.861030  100.000000  \n",
            "4  68.653680   43.616789  \n"
          ]
        }
      ],
      "source": [
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Asset_Number', 'start', 'stop', 'event', 'Category', 'usage',\n",
            "       'temperature', 'load', 'health_bar'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Asset_Number      int64\n",
            "start             int64\n",
            "stop              int64\n",
            "event             int64\n",
            "Category         object\n",
            "usage           float64\n",
            "temperature     float64\n",
            "load            float64\n",
            "health_bar      float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11100, 9)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Drop category column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.drop(columns=['Category'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11100, 8)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Different Data Frame For Individual assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100080 100081 300006 300007 300008 300009 300010 300011 300012 300013\n",
            " 300014 300015 300035 300036 300016 300037 300017 300018 300019 300020\n",
            " 300021 300022 300038 300039 300023 300024 300025 300026 300027 300028\n",
            " 300029 300030 300031 300032 300033 300034 600053 600054 600055 600056\n",
            " 600057 600058 600059 600060 600061 600062 600063 600064 600065 600066\n",
            " 600067 600068 600069 600070 600071 600072 600073 600074 600075 600076\n",
            " 600077 600078 600079 600080 600081 600082 600083 600084 600085 600086\n",
            " 600087 600088 600089 600090 600091 600092 600093 600094 600095 600096\n",
            " 600097 600098 600099 600100 600101 600102 600103 600104 600105 600106\n",
            " 600107 600108 600109 600110 600111 600112 600113 600114 600115 600116\n",
            " 600117 600118 600119 600120 600121 600122 600123 600124 600125 600126\n",
            " 600127]\n"
          ]
        }
      ],
      "source": [
        "unique_assets = df['Asset_Number'].unique()\n",
        "print(unique_assets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "asset_dfs = {}\n",
        "\n",
        "for asset_id in unique_assets:\n",
        "    asset_df = df[df['Asset_Number'] == asset_id]\n",
        "\n",
        "    asset_dfs[asset_id] = asset_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Asset_Number  start  stop  event      usage  temperature       load  \\\n",
            "0         100080      0    14      0  89.242856    21.072078  18.921408   \n",
            "1         100080     14    17      1  18.028311    52.449810  30.837617   \n",
            "2         100080     17    19      1  56.002279    75.497191  14.388191   \n",
            "3         100080     19    21      0  13.719146    12.130803  54.861030   \n",
            "4         100080     21    25      0  34.996827    10.222960  68.653680   \n",
            "..           ...    ...   ...    ...        ...          ...        ...   \n",
            "95        100080    447   448      0  82.623957    79.928071  85.666082   \n",
            "96        100080    448   453      0  52.185868    50.653635  60.664292   \n",
            "97        100080    453   458      1  87.750317    41.481725  98.926973   \n",
            "98        100080    458   473      1  38.249971    97.978681  59.125432   \n",
            "99        100080    473   497      0  92.668396    58.518503  98.768340   \n",
            "\n",
            "    health_bar  \n",
            "0    75.926079  \n",
            "1    46.889084  \n",
            "2    46.312371  \n",
            "3   100.000000  \n",
            "4    43.616789  \n",
            "..         ...  \n",
            "95   68.605929  \n",
            "96   39.658019  \n",
            "97   22.461961  \n",
            "98    0.000000  \n",
            "99   84.151291  \n",
            "\n",
            "[100 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "individual_asset = asset_dfs[100080]\n",
        "print(individual_asset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 8)\n"
          ]
        }
      ],
      "source": [
        "print(individual_asset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CoxTimeVaryingFitter on each asset dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lifelines import CoxTimeVaryingFitter\n",
        "from lifelines.utils import concordance_index\n",
        "\n",
        "def run_ctv_pipeline(asset_df, test_size=20):\n",
        "    if asset_df['event'].sum() == 0:\n",
        "        return None, {\"error\": \"No observed events\"}\n",
        "\n",
        "    if len(asset_df) <= test_size:\n",
        "        return None, {\"error\": \"Insufficient data for train-test split\"}\n",
        "\n",
        "    # Split\n",
        "    train_df = asset_df.iloc[:-test_size].copy()\n",
        "    test_df = asset_df.iloc[-test_size:].copy()\n",
        "\n",
        "    if test_df[\"event\"].sum() == 0:\n",
        "        return None, {\"error\": \"Test set has no events\"}\n",
        "\n",
        "    # Drop NaNs\n",
        "    train_df = train_df.dropna()\n",
        "    test_df = test_df.dropna()\n",
        "\n",
        "    # Check again after dropping\n",
        "    if train_df.empty or test_df.empty:\n",
        "        return None, {\"error\": \"Train or test set empty after dropping NaNs\"}\n",
        "\n",
        "    try:\n",
        "        # Fit with regularization\n",
        "        ctv = CoxTimeVaryingFitter(penalizer=0.1)\n",
        "        ctv.fit(train_df, id_col=\"Asset_Number\", start_col=\"start\", stop_col=\"stop\", event_col=\"event\")\n",
        "\n",
        "        # Predict partial hazard\n",
        "        test_df.loc[:, \"predicted_hazard\"] = ctv.predict_partial_hazard(test_df)\n",
        "\n",
        "        # Evaluate\n",
        "        c_index = concordance_index(\n",
        "            test_df[\"stop\"],\n",
        "            -test_df[\"predicted_hazard\"],\n",
        "            test_df[\"event\"]\n",
        "        )\n",
        "\n",
        "        return ctv, {\n",
        "            \"Concordance_Index\": round(c_index, 4),\n",
        "            \"Train_Size\": len(train_df),\n",
        "            \"Test_Size\": len(test_df)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100080] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[100081] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300006] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300007] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300008] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300009] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300010] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300011] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300012] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300013] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300014] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300015] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300035] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300036] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300016] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300037] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300017] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300018] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300019] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300020] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300021] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300022] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300038] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300039] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300023] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300024] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300025] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300026] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300027] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300028] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300029] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300030] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300031] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300032] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300033] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[300034] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600053] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600054] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600055] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600056] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600057] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600058] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600059] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600060] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600061] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600062] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600063] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600064] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600065] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600066] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600067] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600068] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600069] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600070] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600071] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600072] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600073] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600074] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600075] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600076] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600077] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600078] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600079] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600080] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600081] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600082] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600083] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600084] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600085] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600086] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600087] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600088] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600089] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600090] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600091] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600092] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600093] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600094] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600095] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600096] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600097] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600098] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600099] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600100] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600101] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600102] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600103] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600104] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600105] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600106] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600107] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600108] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600109] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600110] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600111] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600112] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600113] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600114] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600115] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600116] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600117] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600118] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600119] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600120] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600121] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600122] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600123] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600124] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600125] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600126] Skipped: NaNs detected in inputs, please correct or drop.\n",
            "[600127] Skipped: NaNs detected in inputs, please correct or drop.\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "\n",
        "for asset_id, df_asset in asset_dfs.items():\n",
        "    model, metrics = run_ctv_pipeline(df_asset)\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"[{asset_id}] Skipped: {metrics['error']}\")\n",
        "    else:\n",
        "        print(f\"[{asset_id}] ✅ Concordance Index: {metrics['Concordance_Index']}\")\n",
        "        results[asset_id] = {\n",
        "            \"model\": model,\n",
        "            \"metrics\": metrics\n",
        "        }\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
